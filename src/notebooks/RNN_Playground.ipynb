{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4 #size of the hidden unit of a RNN cell\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Basic RNN model:\n",
    "\n",
    "$S_t = tanh ( W . [X_t, S_{t-1}] + b_s)$\n",
    "\n",
    "$O_t = \\sigma (U . S_t + b_o)$\n",
    "\n",
    "where\n",
    "\n",
    "$X_t \\in R^{n}$ is the input\n",
    "\n",
    "$S_t \\in R^{h}$ is the internal (memory) state\n",
    "\n",
    "$O_t \\in R^{m}$ is the output\n",
    "\n",
    "$W \\in W^{h \\times (n + h)}$\n",
    "\n",
    "$U \\in W^{m \\times h}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sample time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    print \"batch_partition_length={}, epoch_size={}\".format(batch_partition_length,epoch_size)\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing RNN with the LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "\"\"\"\n",
    "loss and training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#losses and train_step\n",
    "total_loss = tf.reduce_mean([tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=label) \\\n",
    "                             for logit, label in zip(logits, y_as_list)])\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print \"EPOCH\".format(idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                #print \"Step {} with X={} and Y={}\".format(step, X.shape, Y.shape)\n",
    "                training_loss_, training_state, _ = sess.run([total_loss,\n",
    "                                                              final_state,\n",
    "                                                              train_step],\n",
    "                                                             feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 50 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print \"Average loss at step {} for the last 50 steps: {}\".format(step, training_loss/50)\n",
    "                    training_losses.append(training_loss/50)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH\n",
      "batch_partition_length=5000, epoch_size=1000\n",
      "Average loss at step 50 for the last 50 steps: 0.681035029888\n",
      "Average loss at step 100 for the last 50 steps: 0.634772652388\n",
      "Average loss at step 150 for the last 50 steps: 0.576100723743\n",
      "Average loss at step 200 for the last 50 steps: 0.525952236056\n",
      "Average loss at step 250 for the last 50 steps: 0.520714276433\n",
      "Average loss at step 300 for the last 50 steps: 0.521104617715\n",
      "Average loss at step 350 for the last 50 steps: 0.519392771125\n",
      "Average loss at step 400 for the last 50 steps: 0.518775479198\n",
      "Average loss at step 450 for the last 50 steps: 0.518281936646\n",
      "Average loss at step 500 for the last 50 steps: 0.519793474674\n",
      "Average loss at step 550 for the last 50 steps: 0.515297423601\n",
      "Average loss at step 600 for the last 50 steps: 0.519666594267\n",
      "Average loss at step 650 for the last 50 steps: 0.520122669339\n",
      "Average loss at step 700 for the last 50 steps: 0.519496552348\n",
      "Average loss at step 750 for the last 50 steps: 0.519184584022\n",
      "Average loss at step 800 for the last 50 steps: 0.521013196111\n",
      "Average loss at step 850 for the last 50 steps: 0.520115332007\n",
      "Average loss at step 900 for the last 50 steps: 0.52061815083\n",
      "Average loss at step 950 for the last 50 steps: 0.51995225668\n",
      "EPOCH\n",
      "batch_partition_length=5000, epoch_size=1000\n",
      "Average loss at step 50 for the last 50 steps: 0.531754187942\n",
      "Average loss at step 100 for the last 50 steps: 0.520538605452\n",
      "Average loss at step 150 for the last 50 steps: 0.518436476588\n",
      "Average loss at step 200 for the last 50 steps: 0.516852009296\n",
      "Average loss at step 250 for the last 50 steps: 0.521313527226\n",
      "Average loss at step 300 for the last 50 steps: 0.517346785069\n",
      "Average loss at step 350 for the last 50 steps: 0.519878214002\n",
      "Average loss at step 400 for the last 50 steps: 0.519282488227\n",
      "Average loss at step 450 for the last 50 steps: 0.517081510425\n",
      "Average loss at step 500 for the last 50 steps: 0.520700326562\n",
      "Average loss at step 550 for the last 50 steps: 0.518043136001\n",
      "Average loss at step 600 for the last 50 steps: 0.516653285027\n",
      "Average loss at step 650 for the last 50 steps: 0.520503947139\n",
      "Average loss at step 700 for the last 50 steps: 0.518209870458\n",
      "Average loss at step 750 for the last 50 steps: 0.519037020802\n",
      "Average loss at step 800 for the last 50 steps: 0.515941092372\n",
      "Average loss at step 850 for the last 50 steps: 0.516979671121\n",
      "Average loss at step 900 for the last 50 steps: 0.515879255533\n",
      "Average loss at step 950 for the last 50 steps: 0.52385345757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12274f710>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyP8D5BnDms1b/B0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122692410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(2,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define RNN Model with the LSTM architecture\n",
    "class LSTM_Model():\n",
    "    def __init__(self, num_units, batch_size, learning_rate, train_seq_len,\n",
    "                 vocab_size, infer_sample=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_units:      The number of hidden units in the LSTM cell (same as num_units in BasicLSTMCell)\n",
    "                          or rather it is the size of the state of the LSTM cell. Note: LSTM cell outputs\n",
    "                          two things at each time step: the output h_t and the cell state C_t.\n",
    "              \n",
    "          forget_bias:    The extra bias to be added to forget gates's bias. It is done to in order \n",
    "                          to reduce the scale of forgetting in the beginning of the training.\n",
    "                       \n",
    "          \n",
    "          batch_size:     Number of examples to train on at once\n",
    "          \n",
    "          learning_rate:  \n",
    "          train_seq_len:  The length of the surrounding word group\n",
    "          vocab_size:     \n",
    "          infer_sample:\n",
    "        \"\"\"\n",
    "        self.num_units = num_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.train_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.train_seq_len = train_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.train_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.train_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.num_units, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.num_units],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.train_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.num_units])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.train_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.train_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
