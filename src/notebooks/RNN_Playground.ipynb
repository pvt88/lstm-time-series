{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4 #size of the hidden unit of a RNN cell\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Basic RNN model:\n",
    "\n",
    "$S_t = tanh ( W . [X_t, S_{t-1}] + b_s)$\n",
    "\n",
    "$O_t = \\sigma (U . S_t + b_o)$\n",
    "\n",
    "where\n",
    "\n",
    "$X_t \\in R^{n}$ is the input\n",
    "\n",
    "$S_t \\in R^{h}$ is the internal (memory) state\n",
    "\n",
    "$O_t \\in R^{m}$ is the output\n",
    "\n",
    "$W \\in W^{h \\times (n + h)}$\n",
    "\n",
    "$U \\in W^{m \\times h}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sample time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    print \"batch_partition_length={}, epoch_size={}\".format(batch_partition_length,epoch_size)\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing RNN with the LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "\"\"\"\n",
    "loss and training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#losses and train_step\n",
    "total_loss = tf.reduce_mean([tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=label) \\\n",
    "                             for logit, label in zip(logits, y_as_list)])\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print \"EPOCH\".format(idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                #print \"Step {} with X={} and Y={}\".format(step, X.shape, Y.shape)\n",
    "                training_loss_, training_state, _ = sess.run([total_loss, \\\n",
    "                                                              final_state,\n",
    "                                                              train_step],\n",
    "                                                             feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 50 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print \"Average loss at step {} for the last 50 steps: {}\".format(step, training_loss/50)\n",
    "                    training_losses.append(training_loss/50)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nEPOCH', 0)\n",
      "batch_partition_length=5000, epoch_size=1000\n",
      "Average loss at step 50 for the last 50 steps: 0.623421595097\n",
      "Average loss at step 100 for the last 50 steps: 0.541980254054\n",
      "Average loss at step 150 for the last 50 steps: 0.521099997759\n",
      "Average loss at step 200 for the last 50 steps: 0.520702608824\n",
      "Average loss at step 250 for the last 50 steps: 0.517423157096\n",
      "Average loss at step 300 for the last 50 steps: 0.521258957386\n",
      "Average loss at step 350 for the last 50 steps: 0.520309259892\n",
      "Average loss at step 400 for the last 50 steps: 0.52043048799\n",
      "Average loss at step 450 for the last 50 steps: 0.518798231483\n",
      "Average loss at step 500 for the last 50 steps: 0.521707246304\n",
      "Average loss at step 550 for the last 50 steps: 0.519877796173\n",
      "Average loss at step 600 for the last 50 steps: 0.523264604211\n",
      "Average loss at step 650 for the last 50 steps: 0.518482660651\n",
      "Average loss at step 700 for the last 50 steps: 0.51692381084\n",
      "Average loss at step 750 for the last 50 steps: 0.52176653564\n",
      "Average loss at step 800 for the last 50 steps: 0.51788041532\n",
      "Average loss at step 850 for the last 50 steps: 0.520009177923\n",
      "Average loss at step 900 for the last 50 steps: 0.517343226075\n",
      "Average loss at step 950 for the last 50 steps: 0.519320549369\n",
      "('\\nEPOCH', 1)\n",
      "batch_partition_length=5000, epoch_size=1000\n",
      "Average loss at step 50 for the last 50 steps: 0.53269919157\n",
      "Average loss at step 100 for the last 50 steps: 0.516760588884\n",
      "Average loss at step 150 for the last 50 steps: 0.517960262299\n",
      "Average loss at step 200 for the last 50 steps: 0.519746870399\n",
      "Average loss at step 250 for the last 50 steps: 0.519851666093\n",
      "Average loss at step 300 for the last 50 steps: 0.518745984435\n",
      "Average loss at step 350 for the last 50 steps: 0.520561653376\n",
      "Average loss at step 400 for the last 50 steps: 0.52056550324\n",
      "Average loss at step 450 for the last 50 steps: 0.518685011864\n",
      "Average loss at step 500 for the last 50 steps: 0.517325215936\n",
      "Average loss at step 550 for the last 50 steps: 0.513951081634\n",
      "Average loss at step 600 for the last 50 steps: 0.520132023692\n",
      "Average loss at step 650 for the last 50 steps: 0.518413282037\n",
      "Average loss at step 700 for the last 50 steps: 0.518450558186\n",
      "Average loss at step 750 for the last 50 steps: 0.522996003628\n",
      "Average loss at step 800 for the last 50 steps: 0.518240414858\n",
      "Average loss at step 850 for the last 50 steps: 0.520265027881\n",
      "Average loss at step 900 for the last 50 steps: 0.515845627785\n",
      "Average loss at step 950 for the last 50 steps: 0.517471472025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1225bc990>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12250add0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(2,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
