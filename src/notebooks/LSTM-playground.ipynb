{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import csv\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "DEFAULTS = [[0.0] for x in xrange(0, SEQ_LEN)]\n",
    "BATCH_SIZE = 20\n",
    "TIMESERIES_COL = 'rawdata'\n",
    "N_OUTPUTS = 2  # in each sequence, 1-8 are features, and 9-10 is label\n",
    "N_INPUTS = SEQ_LEN - N_OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sample time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_time_series():\n",
    "    freq = (np.random.random()*0.5) + 0.1  # 0.1 to 0.6\n",
    "    ampl = np.random.random() + 0.5  # 0.5 to 1.5\n",
    "    x = np.sin(np.arange(0,SEQ_LEN) * freq) * ampl\n",
    "    return x\n",
    "\n",
    "def to_csv(filename, N):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    for lineno in xrange(0, N):\n",
    "      seq = create_time_series()\n",
    "      line = \",\".join(map(str, seq))\n",
    "      ofp.write(line + '\\n')\n",
    "\n",
    "to_csv('train.csv', 1000)  # 1000 sequences\n",
    "to_csv('valid.csv',  50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode=tf.contrib.learn.ModeKeys.TRAIN):  \n",
    "    def _input_fn():\n",
    "        num_epochs = 100 if mode == tf.contrib.learn.ModeKeys.TRAIN else 1\n",
    "\n",
    "        # could be a path to one file or a file pattern.\n",
    "        input_file_names = tf.train.match_filenames_once(filename)\n",
    "\n",
    "        filename_queue = tf.train.string_input_producer(\n",
    "            input_file_names, num_epochs=num_epochs, shuffle=True)\n",
    "        reader = tf.TextLineReader()\n",
    "        _, value = reader.read_up_to(filename_queue, num_records=BATCH_SIZE)\n",
    "\n",
    "        value_column = tf.expand_dims(value, -1)\n",
    "        print 'readcsv={}'.format(value_column)\n",
    "\n",
    "        # all_data is a list of tensors\n",
    "        all_data = tf.decode_csv(value_column, record_defaults=DEFAULTS)  \n",
    "        inputs = all_data[:len(all_data)-N_OUTPUTS]  # first few values\n",
    "        label = all_data[len(all_data)-N_OUTPUTS : ] # last few values\n",
    "\n",
    "        # from list of tensors to tensor with one more dimension\n",
    "        inputs = tf.concat(inputs, axis=1)\n",
    "        label = tf.concat(label, axis=1)\n",
    "        print 'inputs={}'.format(inputs)\n",
    "\n",
    "        return {TIMESERIES_COL: inputs}, label   # dict of features, label\n",
    "    return _input_fn\n",
    "\n",
    "def get_train_d():\n",
    "    return read_dataset('train.csv', mode=tf.contrib.learn.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid():\n",
    "    return read_dataset('valid.csv', mode=tf.contrib.learn.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing RNN with the LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_SIZE = 3  # number of hidden layers in each of the LSTM cells\n",
    "\n",
    "# create the inference model\n",
    "def simple_rnn(inputs):\n",
    "    # Reshape input shape to become a sequence\n",
    "    x = tf.split(inputs[TIMESERIES_COL], N_INPUTS, 1)\n",
    "    print 'x={}'.format(x)\n",
    "\n",
    "    # Configure the RNN\n",
    "    lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)\n",
    "    \n",
    "    # We only need the output and can ignore the state\n",
    "    outputs, _ = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # slice to keep only the last cell of the RNN\n",
    "    last_output = outputs[-1]\n",
    "    print 'last outputs={}'.format(last_output)\n",
    "\n",
    "    # output is result of linear activation of last layer of RNN\n",
    "    weight = tf.Variable(tf.random_normal([LSTM_SIZE, N_OUTPUTS]))\n",
    "    bias = tf.Variable(tf.random_normal([N_OUTPUTS]))\n",
    "    predictions = tf.matmul(last_output, weight) + bias\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define RNN Model with the LSTM architecture\n",
    "class LSTM_Model():\n",
    "    def __init__(self, num_units, batch_size, learning_rate, train_seq_len,\n",
    "                 vocab_size, infer_sample=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_units:      The number of hidden units in the LSTM cell (same as num_units in BasicLSTMCell)\n",
    "                          or rather it is the size of the state of the LSTM cell. Note: LSTM cell outputs\n",
    "                          two things at each time step: the output h_t and the cell state C_t.\n",
    "              \n",
    "          forget_bias:    The extra bias to be added to forget gates's bias. It is done to in order \n",
    "                          to reduce the scale of forgetting in the beginning of the training.\n",
    "                       \n",
    "          \n",
    "          batch_size:     Number of examples to train on at once\n",
    "          \n",
    "          learning_rate:  \n",
    "          train_seq_len:  The length of the surrounding word group\n",
    "          vocab_size:     \n",
    "          infer_sample:\n",
    "        \"\"\"\n",
    "        self.num_units = num_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.train_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.train_seq_len = train_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.train_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.train_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.num_units, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.num_units],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.train_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.num_units])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.train_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.train_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
